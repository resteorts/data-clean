
---
title: "Module 5: Probabilistic Blocking, Part II"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---



Agenda
===

- Data Cleaning Pipeline
- Blocking
- Locality Sensitive Hashing (LSH)
- Hash functions
- Hashed shingles
- Signatures
- Characteristic Matrix
- Minhash (Jaccard Similarity Approximation)
- Back to LSH

Load R packages
===

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(RecordLinkage)
library(blink)
library(knitr)
library(textreuse) # text reuse/document similarity
library(tokenizers) # shingles
library(devtools)
library(cora)
library(ggplot2)
# install_github("resteorts/cora")
data(cora) # load the cora data set
data(cora_gold) # contains the cora unique identifiers
```



<!-- Data Cleaning Pipeline -->
<!-- === -->

<!-- \begin{figure} -->
<!--   \begin{center} -->
<!--     \includegraphics[width=\textwidth]{finalFigures/pipeline} -->
<!--     \caption{Data cleaning pipeline.} -->
<!--     \end{center} -->
<!-- \end{figure} -->

<!-- Blocking -->
<!-- === -->

<!-- \begin{figure} -->
<!--   \begin{center} -->
<!--     \includegraphics[width=\textwidth]{finalFigures/block.png} -->
<!--     \caption{Left: All to all record comparison. Right: Example of resulting blocking partitions. } -->
<!--     \end{center} -->
<!-- \end{figure} -->


LSH
===

Locality sensitive hashing (LSH) is a fast method of blocking for record linkage that orginates from the computer science literature. 

LSH
===

\begin{itemize}
\item LSH tries to preserve similarity after dimension reduction.
\begin{itemize}
\item What kind of similarity? $\leftrightarrow$ What kind of dimension reduction?
\end{itemize}
\end{itemize}


\small
```{r, cache=TRUE, echo=FALSE}
# get only the columns we want
# number of records
n <- nrow(cora) 
# create id column
dat <- data.frame(id = seq_len(n)) 
# get columns we want
dat <- cbind(dat, cora[, c("title", "authors", "journal")]) 
shingles <- apply(dat, 1, function(x) {
  # tokenize strings
  tokenize_character_shingles(paste(x[-1], collapse=" "), n = 3)[[1]]
})
# empty holder for similarities
jaccard <- expand.grid(record1 = seq_len(n), 
                       record2 = seq_len(n))

# don't need to compare the same things twice
jaccard <- jaccard[jaccard$record1 < jaccard$record2,]
```


# Hash function overview

- Traditionally, a *hash function* maps objects to integers such that similar objects are far apart

\vspace*{1em}

- Instead, we will use a special hash function that does the **opposite** of this, i.e., similar objects are placed closed together!

\vspace*{1em} Technical reading on this: Chen et al. (2018) and Shrivastava and Steorts (2018)

# Hash function 


We are looking for a hash function $h()$ such that
\begin{itemize}
\item if $\text{sim}(A,B)$ is high, then with high prob.\ $h(A) = h(B).$
\item if $\text{sim}(A,B)$ is low, then with high prob.\ $h(A) \neq h(B).$
\end{itemize}



# Hashing shingles

\begin{enumerate}
\item Instead of storing the strings as shingles, we store the \textbf{hashed values}.
\item These are integers (instead of strings). 
\end{enumerate}

We do this because the integers take up less memory, so we are performing a type of **dimension reduction**. 



```{r hash-tokens, echo=TRUE, cache=TRUE, echo=FALSE}
# instead store hash values (less memory)
hashed_shingles <- apply(dat, 1, function(x) {
  # get the string
  string <- paste(x[-1], collapse=" ") 
  shingles <- 
    # 3-shingles
    tokenize_character_shingles(string, n = 3)[[1]] 
  # return hashed shingles
  hash_string(shingles) 
})
```


```{r hash-tokens-jaccard, cache=TRUE, echo=FALSE}
# Jaccard similarity on hashed shingles
hashed_jaccard <- 
  expand.grid(record1 = seq_len(n), record2 = seq_len(n))

# don't need to compare the same things twice
hashed_jaccard <- 
  hashed_jaccard[hashed_jaccard$record1 
                 < hashed_jaccard$record2,]
# see how long this takes
time <- Sys.time() 
hashed_jaccard$similarity <- 
  apply(hashed_jaccard, 1, function(pair) {
  jaccard_similarity(hashed_shingles[[pair[1]]], 
                     hashed_shingles[[pair[2]]])
}) # get jaccard for each hashed pair
time <- difftime(Sys.time(), time, units = "secs") 
```

# Hashing shingles (continued)


To store the shingles, it took $`r object.size(shingles)`$ bytes. 

\vspace*{1em}

To hash the shingles, it took $`r object.size(hashed_shingles)`$ bytes. 

\vspace*{1em}

Thus, we will **hash shingles** because it takes up less memory.  


\vspace*{1em}

\textbf{The entire pairwise comparison still took the same amount of time ($\approx `r round(time/(60), 2)`$ minutes) for both approaches, so keep in mind we have not improved this aspect of our approach, but we will improve upon this later!}








```{r, echo = FALSE}
# return if an item is in a list
item_in_list <- function(item, list) {
  as.integer(item %in% list) 
}
```


```{r characteristic, cache=TRUE, echo=FALSE}
# get the characteristic matrix
# items are all the unique hash values
# columns will be each record
# we want to keep track of where each hash is included 
char_mat <- data.frame(item = unique(unlist(hashed_shingles)))
# for each hashed shingle, see if it is in each row
contained <- lapply(hashed_shingles, function(col) {
  vapply(char_mat$item, FUN = item_in_list, 
         FUN.VALUE = integer(1), list = col)
})
# list to matrix
char_mat <- do.call(cbind, contained) 
# row names
rownames(char_mat) <- unique(unlist(hashed_shingles)) 
# column names
colnames(char_mat) <- paste("Record", seq_len(nrow(dat))) 
```

# Characteristic matrix (continued)

We can visualize the records (columns) and the hashed shingles in a large, binary **characteristic matrix**

We can think of the fact that we have transformed the input (original data) into the **characteristic matrix**

# Characteristic matrix (continued)
```{r characteristic-2, cache=TRUE}
# inspect results
kable(char_mat[10:15, 1:5])
```

# Similarity preserving summaries of sets

Sets of shingles are large (larger than the original data set)
\vfill

If we have millions of records in our data set, it may not be possible to store all the shingle-sets in memory
\vfill

We can replace large sets by smaller representations, called *signatures*
\vfill

We can use the \emph{signatures} to **approximate** Jaccard similarity (using a cool fact)
\vfill


The result is a $`r dim(char_mat)[1]`\times `r dim(char_mat)[2]`$ matrix

**Question: Why is storing the data in this way not a good idea**?



# Big Idea

Let's apply a permutation to the charateristic matrix:

\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/permuted-matrix}
    % Source: Original work by RCS
  \end{center}
  \caption{Consider applying The $\pi$ vector to the input (charateristic matrix) which provides the permuted matrix.}
  \label{figure:permute}
\end{figure}	

**Why are we performing this permutation?**


# Now apply the Minhash

We want to create the signature matrix through minhashing

1. Permute the rows of the characteristic matrix $m$ times
\vspace{.2in}
2. Iterate over each column of the permuted matrix 
\vspace{.2in}
3. Populate the signature matrix, row-wise, with the row index from the first `1` value found in the column 

The signature matrix is a hashing of values from the permuted characteristic matrix and has one row for the number of permutations calculated ($m$), and a column for each record.



# Signature Matrix

The resulting signature matrix of the permuted matrix is

\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{figures/permuted-matrix}
    % Source: Original work by RCS
  \end{center}
  \caption{Consider applying The $\pi$ vector to the input (charateristic matrix) which provides the permuted matrix.}
\end{figure}

```{r}
signature.matrix <- c(2,4,3,1)
```

# Signature matrix and Jaccard similarity

The relationship between the random permutations of the characteristic matrix and the Jaccard Similarity is
$$
Pr\{\min[\pi(A)] = \min[\pi(B)]\} = \frac{|A \cap B|}{|A \cup B|}
$$

\vfill

We use this relationship to **approximate** the similarity between any two records 

\vfill
We look down each column of the signature matrix, and compare it to any other column

\vfill
The number of agreements over the total number of combinations is an approximation to Jaccard measure

\vfill

# Proof

Assume that $\pi$ is a random permutation. Then
$$\text{Pr} \{ \min[\pi(A)] = \min[\pi(B)] \} = 
\frac{|A \cap B   |           }{  |A \cup B|      }.$$

\textbf{Proof}:
Suppose $X$ is a record (set of shingles). Let $y \in X$ be a shingle.
$\text{Pr} [ \pi(y) = \min \pi( X)  ] = 1/|X|.$\footnote{It's equally likely that any $y \in X$ is mapped to the min element.} 

Let $y$ be such that $\pi(y) = \min (\pi(A \cap B)).$ Then
\begin{itemize}
\item $\pi(y) = \min (\pi(A)) $ if $y \in A$ OR
\item $\pi(y) = \min (\pi(B)) $ if $y \in B.$
\end{itemize}
The probability that both are true is $Pr(y \in A \cap B).$
This implies that 
$$\text{Pr} \{ \min(\pi(A)) = \min(\pi(B)) \} = 
\frac{|A \cap B   |           }{  |A \cup B|      }.$$


# Signature Matrix

**Using the relationship between the Jaccard similarity and the signature matrix, do any records agree? Explain.**



```{r minhash-1, cache=TRUE ,echo=FALSE}
# set seed for reproducibility
set.seed(02082018)
# function to get signature for 1 permutation
get_sig <- function(char_mat) {
  # get permutation order
  permute_order <- sample(seq_len(nrow(char_mat)))
  # get min location of "1" for each column (apply(2, ...))
  t(apply(char_mat[permute_order, ], 2, 
          function(col) min(which(col == 1))))
}
# repeat many times
m <- 360
sig_mat <- matrix(NA, nrow=m, 
                  ncol=ncol(char_mat)) #empty matrix
for(i in 1:m) {
  sig_mat[i, ] <- get_sig(char_mat) #fill matrix
}
colnames(sig_mat) <- colnames(char_mat) #column names
```


```{r minhash-2, cache=TRUE}
# inspect results
kable(sig_mat[1:10, 1:5])
```


# Jaccard similarity approximation
\small
```{r jaccard-sig, fig.height=4, cache=TRUE}
# add jaccard similarity approximated from the minhash to compare
# number of agreements over the total number of combinations
hashed_jaccard$similarity_minhash <- 
  apply(hashed_jaccard, 1, function(row) {
  sum(sig_mat[, row[["record1"]]] 
      == sig_mat[, row[["record2"]]])/nrow(sig_mat)
})

# how far off is this approximation? plot differences
qplot(hashed_jaccard$similarity_minhash - hashed_jaccard$similarity) +
  xlab("Difference between Jaccard similarity and minhash approximation")
```

# Jaccard similarity approximation

```{r jaccard-sig-again, fig.height=4, cache=TRUE, echo=FALSE, warning=FALSE}
# add jaccard similarity approximated from the minhash to compare
# number of agreements over the total number of combinations
hashed_jaccard$similarity_minhash <- 
  apply(hashed_jaccard, 1, function(row) {
  sum(sig_mat[, row[["record1"]]] == 
        sig_mat[, row[["record2"]]])/nrow(sig_mat)
})

# how far off is this approximation? plot differences
qplot(hashed_jaccard$similarity_minhash - hashed_jaccard$similarity) +
  xlab("Difference between Jaccard similarity and minhash approximation") 
```

Used minhashing to get an approximation to the Jaccard similarity, which helps by allowing us to store less data (hashing) and avoid storing sparse data (signature matrix)

# Wait did I miss something?

We still haven not addressed the issue of **pairwise comparisons** but we have address the issue of storing things **more efficiently**!

# Locality Sensitive Hashing (LSH) to the Rescue

We want to hash items several times such that similar items are more likely to be hashed into the same bucket.

1. Divide the \textbf{signature matrix} into $b$ bands with $r$ rows each so $m = b * r$ where *m* is the number of times that we drew a permutation of the characteristic matrix in the process of minhashing
2. Each band is hashed to a bucket by comparing the minhash for those permutations 
    - If they match within the band, then they will be hashed to the same bucket
3. \textbf{If two documents are hashed to the same bucket they will be considered candidate pairs}
  
We only check *candidate pairs* for similarity

# Banding and buckets

```{r banding, results='asis'}
# view the signature matrix
print(xtable::xtable(sig_mat[1:10, 1:5]), 
      hline.after = c(-1,0,5,10), comment=F)
```

<!-- # Tuning -->

<!-- ## How to choose $k$ -->

<!-- How large $k$ should be depends on how long our data strings are -->

<!-- The important thing is $k$ should be picked large enough such that the probability of any given shingle is *low* -->

<!-- ## How to choose $b$ -->

<!-- $b$ must divide $m$ evenly such that there are the same number of rows $r$ in each band  -->



# Choosing $b$

\vspace{-.2in}
\scriptsize
$$
P(\text{two documents w/ Jaccard similarity } s \text{ marked as potential match}) = 1 - (1 - s^{m/b})^b
$$

\normalsize

```{r inclusion-probs, fig.cap=paste0("Probability that a pair of documents with a Jaccard similarity $s$ will be marked as potential matches for various bin sizes $b$ for $s = .25, .75$ for the number of permutations we did, $m = ", m, "$."), fig.height=3, echo=FALSE}
# library to get divisors of m
library(numbers) 

# look at probability of binned together for various bin sizes and similarity values
bin_probs <- expand.grid(s = c(.25, .75), h = m, b = divisors(m))
bin_probs$prob <- apply(bin_probs, 1, function(x) lsh_probability(x[["h"]], x[["b"]], x[["s"]]))


# plot as curves
ggplot(bin_probs) +
  geom_line(aes(x = prob, y = b, colour = factor(s), group = factor(s)), size = 2) +
  geom_point(aes(x = prob, y = b, colour = factor(s)), size = 3) +
  xlab("Probability") +
  scale_color_discrete("s")
  
```

For $b = 90$, a pair of records with Jaccard similarity $.25$ will have a `r scales::percent(bin_probs[bin_probs$b == 90 & bin_probs$s == .25, "prob"])` chance of being matched as candidates and a pair of records with Jaccard similarity $.75$ will have a `r scales::percent(bin_probs[bin_probs$b == 90 & bin_probs$s == .75, "prob"])` chance of being matched as candidates

# "Easy" LSH in R

There an easy way to do LSH using the built in functions in the `textreuse` package via the functions `minhash_generator` and `lsh` (so we don't have to perform it by hand): 


```{r show-package-lsh, echo=TRUE, cache=TRUE}
# choose appropriate num of bands
b <- 90

# create the minhash function
minhash <- minhash_generator(n = m, seed = 02082018)
```

# "Easy" LSH in R (Continued)

\small
```{r show-package-lsh-1, echo=TRUE, cache=TRUE}
# build the corpus using textreuse
docs <- apply(dat, 1, function(x) paste(x[-1], collapse = " ")) # get strings
names(docs) <- dat$id # add id as names in vector
corpus <- TextReuseCorpus(text = docs, # dataset
                          tokenizer = tokenize_character_shingles, n = 3, simplify = TRUE, # shingles
                          progress = FALSE, # quietly
                          keep_tokens = TRUE, # store shingles
                          minhash_func = minhash) # use minhash


```

# "Easy" LSH in R (Continued)

```{r show-package-lsh-2, echo=TRUE, cache=TRUE}

# perform lsh to get buckets
buckets <- lsh(corpus, bands = b, progress = FALSE)

# grab candidate pairs
candidates <- lsh_candidates(buckets)

# get Jaccard similarities only for candidates
lsh_jaccard <- lsh_compare(candidates, corpus, 
                           jaccard_similarity, progress = FALSE)
```

# "Easy" LSH in R (cont'd)

```{r, lsh-plot,echo=FALSE}
# plot jaccard similarities that are candidates
qplot(lsh_jaccard$score)
```

# Putting it all together

The last thing we need is to go from candidate pairs to blocks


```{r, echo=TRUE}
library(igraph) #graph package
# think of each record as a node
# there is an edge between nodes if they are candidates
```

# Putting it all together
\small
```{r, echo=TRUE, cache=TRUE}
g <- make_empty_graph(n, directed = FALSE) # empty graph
g <- add_edges(g, is.vector((candidates[, 1:2]))) # candidate edges
g <- set_vertex_attr(g, "id", value = dat$id) # add id

# get custers, these are the blocks
clust <- components(g, "weak") # get clusters
blocks <- data.frame(id = V(g)$id, # record id
                     block = clust$membership) # block number  
head(blocks)
```

# Precision

```{r}
precision <- function(block.labels, IDs) {
  ct = xtabs(~block.labels+IDs)
  
  # Number of true positives
  TP = sum(choose(ct, 2))
  
  # Number of positives = TP + FP
  P = sum(choose(rowSums(ct), 2)) 
  
  return(TP/P)
}
```

# Pairwise Recall


```{r}
recall <- function(block.labels, IDs) {
  ct = xtabs(~IDs+block.labels)
  
  # Number of true positives
  TP = sum(choose(ct, 2))
  
  # Number of true links = TP + FN
  TL = sum(choose(rowSums(ct), 2))
  
  return(TP/TL)
}
```

# Calculation
```{r}
head(blocks$block)
head(blocks$id)
recall(block.labels = blocks$block, IDs  = blocks$id)
precision(block.labels = blocks$block, IDs  = blocks$id) 
library(clevr)
pred_labels = rbind(blocks$id,blocks$block)
true_labels = rbind(cora_gold$id1, cora_gold$id2)
```

# More Calculation
```{r}
dim(pred_labels)
dim(true_labels)
#precision_pairs(true_labels, pred_labels)
```



# Your turn

Using the `fname_c1` and `lname_c1` columns in the `RecordLinkage::RL500` dataset, 

1. Use LSH to get candidate pairs for the dataset
  - What $k$ to use for shingling?
  - What $b$ to use for bucket size?
  
2. Append the blocks to the original dataset as a new column, `block`

# Even faster?

(**fast**): In minhashing we have to perform $m$ permutations to create multiple hashes
\vfill
(**faster**): We would like to reduce the number of hashes we need to create -- "Densified" One Permutation Hashing (DOPH)
\vfill

- One permutation of the signature matrix is used
- The feature space is then binned into $m$ evenly spaced bins
- The $m$ minimums (for each bin separately) are the $m$ different hash values

\vfill


