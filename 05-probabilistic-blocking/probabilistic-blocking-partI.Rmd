
---
title: "Module 5: Probabilistic Blocking, Part I"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---



Agenda
===

- Data Cleaning Pipeline
- Blocking
- Probabilistic Blocking
- Locality Sensitive Hashing (LSH)
- Jaccard Similarity
- Shingling
- Putting it together
- Limitations

Load R packages
===

```{r}
if (!require("pacman")) {
  install.packages("pacman")
  library(pacman)
} 
p_load(RecordLinkage,blink,knitr,cora,ggplot2,textreuse,tokenizers)
```

# ```{r, echo=FALSE, message=FALSE, warning=FALSE}
# library(RecordLinkage)
# library(blink)
# library(knitr)
# library(cora)
# library(ggplot2)
# ```

Data Cleaning Pipeline
===

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{finalFigures/pipeline}
    \caption{Data cleaning pipeline.}
    \end{center}
\end{figure}

Blocking
===

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{finalFigures/block.png}
    \caption{Left: All to all record comparison. Right: Example of resulting blocking partitions. }
    \end{center}
\end{figure}


LSH
===

Locality sensitive hashing (LSH) is a fast method of blocking for record linkage that orginates from the computer science literature. 

# Finding similar records

Our goal is to find *similar* records, where the records are assumed to be strings 
\vfill

How do we define *similar*?
\vfill

# Jaccard similarity



We will work with the  *Jaccard similarity*:
$$
Jac(S, T) = \frac{\mid S \cap T\mid}{\mid S \cup T \mid}.
$$

\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{finalFigures/jaccard}
\caption{Two sets S and T with Jaccard similarity 3/7. The two sets share 3 elements in common, and there are 7 elements in total.}
\label{fig:jaccard}
\end{figure}

# How to represent data as sets?

We want to talk about the similarity of our data (records)$\Rightarrow$ we need to compare sets of records!

- We can construct a set of **short strings** from the data
\vfill
- This is useful because similar datasets will have many common elements (common short strings)
\vfill
- We can do construct these short strings using *shingling*
\vfill

# $k$-shingling (how-to)

1. Think of the data set as a string of characters
\vfill
2. A $k$-shingle (k-gram) is any sub-string (word) of length $k$ found within the a record of the data set
\vfill
3. Associate with each data set the set of $k$-shingles that appear one or more times 
\vfill

# Let's try

Suppose our data set is the string "Hello world", then 

- the set of $2$-shingles is $\{\text{he, el, ll, lo, ow, wo, or, rl, ld}\}$
\vfill
- the set of $3$-shingles is $\{\text{hel, ell, llo, low, owo, wor, orl, rld}\}$
\vfill

# Your turn

We have the following two records:
```{r your-turn1}
# load RL data
data("RLdata500")

# select only 2 records
records <- RLdata500[129:130, c(1,3)]
names(records) <- c("First name", "Last name")

# inspect records
kable(records)
```

# Your turn (continued)

1. Compute the $2$-shingles for each record
\vfill
2. Using Jaccard similarity, how similar are they?
\vfill
3. What do you learn from this exercise? 

# Your turn solution

**Do this on your own and compare with a partner.**

# Your turn solution
\vfill
1. The $2$-shingles for the first record are $\{\text{mi, ic, ch, ha, ae, el, lv, vo, og, ge, el}\}$ and for the second are $\{\text{mi, ic, ch, ha, ae, el, lm, me, ey, ye, er}\}$
\vfill
2. There are 6 items in common $\{\text{mi, ic, ch, ha, ae, el}\}$ and 15 items total $\{\text{mi, ic, ch, ha, ae, el, lv, vo, og, ge, lm, me, ey, ye, er}\}$, so the Jaccard similarity is $\frac{6}{15} = \frac{2}{5} = `r 6/15`$
\vfill
3. You should have learned that this is very tedious to do by hand!

# Useful packages/functions in `R`

From the exercise, you should have learned that we don't want to do this by hand!

Here are some useful packages in `R` that can help us!

\vline

```{r helpful-packages, echo=TRUE, message=FALSE, warning=FALSE}
library(textreuse) # text reuse/document similarity
library(tokenizers) # shingles
```



# Shingling

We can use the following functions to create $k$-shingles and calculate Jaccard similarity for our data

\vline

```{r helpful-functions, eval=FALSE, echo=TRUE}
# get k-shingles
tokenize_character_shingles(x, n)

# calculate jaccard similarity for two sets
jaccard_similarity(a, b) 
```

# Your turn (solution)


```{r}
# create shingles for both names
token.1 <- 
  tokenize_character_shingles("MICHAELVOGEL", n=2)
token.2 <- 
  tokenize_character_shingles("MICHAELMEYER", n=2)
# compute jaccard similarity 
jaccard_similarity(unlist(token.1),unlist(token.2))
```

# Citation Data Set

Research paper headers and citations, with information on authors, title, institutions, venue, date, page numbers and several other fields.

# Citation Data Set

```{r load-ex-data-2, echo=TRUE, message=FALSE, warning=FALSE}
data(cora) # load the cora data set
str(cora) # structure of cora
```

# Your turn 

Using the `title`, `authors`, and `journal` fields in the `cora` dataset,

\vfill
1. Get the $3$-shingles for each record (**hint:** use `tokenize_character_shingles`)
\vfill
2. Obtain the Jaccard similarity between each pair of records (**hint:** use `jaccard_similarity`)
\vfill

# Your turn (partial solution)
\small
```{r your-turn-sol-0, echo=TRUE, cache=TRUE}
# get only the columns we want 
# number of records 
n <- nrow(cora)  
# create id column 
dat <- data.frame(id = seq_len(n))  
# get columns we want 
dat <- cbind(dat, cora[, c("title", "authors", "journal")]) 
```

# Your turn (partial solution)

**Finish the rest of the exercise with a partner, which can be found after the summar**


# Summary

For a data set of size $n$, the number of comparisons we must compute is $$\frac{n(n-1)}{2}.$$

\vfill
For our set of records, we needed to compute $`r scales::comma(nrow(dat)*(nrow(dat) - 1)/2)`$ comparisons
\vfill
For very large data sets, we need something faster (where we filter out records that are not similar).
\vfill
A better approach for data sets of any realistic size is to use *hashing*, which we will look at next time. 
\vfill

# Your turn (Full solution)

\small
```{r your-turn-sol-1, echo=TRUE, cache=TRUE}
# 1. paste the columns together and tokenize for each record
shingles <- apply(dat, 1, function(x) {
  # tokenize strings
  tokenize_character_shingles(paste(x[-1], collapse=" "), n = 3)[[1]]
})
```

# Your turn (Full solution)

\small
```{r your-turn-sol-2, echo=TRUE, cache=TRUE}
# 2. Jaccard similarity between pairs
# empty holder for similarities
jaccard <- expand.grid(record1 = seq_len(n), 
                       record2 = seq_len(n))
# don't need to compare the same things twice
jaccard <- jaccard[jaccard$record1 < jaccard$record2,]
time <- Sys.time() # for timing comparison
jaccard$similarity <- apply(jaccard, 1, function(pair) {
  # get jaccard for each pair
  jaccard_similarity(shingles[[pair[1]]], shingles[[pair[2]]]) 
})
# timing
time <- difftime(Sys.time(), time, units = "secs") 
```

\normalsize
This took took $`r round(time, 2)`$ seconds $\approx `r round(time/(60), 2)`$ minutes

# Your turn (solution, cont'd)

```{r your-turn2-plot, fig.cap="Jaccard similarity for each pair of records. Light blue indicates the two records are more similar and dark blue indicates less similar."}
# plot the jaccard similarities for each pair of records
ggplot(jaccard) +
  geom_raster(aes(x = record1, y = record2, 
                  fill=similarity)) +
  theme(aspect.ratio = 1) +
  scale_fill_gradient("Jaccard similarity") +
  xlab("Record id") + ylab("Record id")
```

# Your turn (solution, cont'd)

```{r your-turn2-plot-again, fig.cap="Jaccard similarity for each pair of records. Light blue indicates the two records are more similar and dark blue indicates less similar.", echo=FALSE}
# plot the jaccard similarities for each pair of records
ggplot(jaccard) +
  geom_raster(aes(x = record1, y = record2, 
                  fill=similarity)) +
  theme(aspect.ratio = 1) +
  scale_fill_gradient("Jaccard similarity") +
  xlab("Record id") + ylab("Record id") +
  theme(plot.margin = margin(5,.8,5,.8, "cm"))
```



